{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da3f427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import hm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import library from gensim  \n",
    "from gensim.models import CoherenceModel\n",
    "from gsdmm import MovieGroupProcess\n",
    "\n",
    "my_doc=pd.read_csv('C:/Users/Kois/lemmatizedCorpus.csv') #Lemmatized tokens\n",
    "my_doc=my_doc.drop(['Unnamed: 0.1'], axis = 1)\n",
    "my_doc=my_doc.drop(['Unnamed: 0'], axis = 1)\n",
    "my_doc=my_doc.to_numpy().tolist()\n",
    "len(my_doc)\n",
    "my_doc\n",
    "\n",
    "new_list=[]\n",
    "temp_list=[]\n",
    "for x in my_doc:\n",
    "    for item in x:\n",
    "        if str(item) != 'nan':\n",
    "            temp_list.append(item)\n",
    "    new_list.append(temp_list)\n",
    "    temp_list=[]\n",
    "my_doc=new_list\n",
    "len(my_doc)\n",
    "\n",
    "stop_words=['እኔ','የእኔ','እኔራሴ','እኛ','የእኛ','እኛራሳችን','አንቺ','ነህ','አላችሁ','እርስዎ','ትፈልጋለህ','ያንተ','ራስህን','እራሳችሁ','እሱ','የእሱ','ራሱ','እሷ','እሷናት','የእሷ','እራሷ','ነው','እነሱ','እነሱን','የእነሱ','ራሳቸው','ምንድን','የትኛው','ማን','ይህ','የሚልነው','ያ','እነዚህ','እነዚያ','ነኝ','ናቸው','ነበር','ነበሩ','ሁን','ቆይቷል','መሆን','አላቸው','አለው','ነበረው','ያለው','መስራት','ያደርጋል','አደረገ','ማድረግ','ሀ','አንድ','የ','እና','ከሆነ','ወይም'\n",
    ",'ምክንያቱም','እንደ','እስከ','እያለ','በ','ለ','ጋር','ስለ','ላይ','መካከል','ወደ','በኩል','ወቅት','ከዚህበፊት','በኋላ','ከላይ','ከታች','ከ','ወደላይ','ታች','ውስጥ','ውጭ','በላይ','እንደገና','ተጨማሪ','ከዚያ','አንድጊዜ','እዚህ','እዚያ','መቼ','የት','ለምን','እንዴት','ሁሉም','ማንኛውም','ሁለቱም','እያንዳንዳቸው','ጥቂቶች','በጣም','ሌላ','አንዳንድ','እንደዚህ','ብቻ','የራሱ','ተመሳሳይ','ስለዚህ','ይልቅ','እንዲሁ','ት','ይችላል','ይገባል','ይገባኛል','አሁን','መ','ም','ኦ','ዳግም','መሆን','ሁለ','ሁለም','ሕዝብ','ሀሙስ','ለመሆኑ','ለምንድን','ሌሎች','መጽሀፍ','ማክሰኞ','ምን','ሰኞ','ሰው','ሲሆን','ስንት','ረቡእ','ቅዳሜ','በዚህ','ብላ','ነገር','አለ','አርብ','አንተ','አንዳንድ','ኢትዮጵያ','እሁድ','እናንተ','እንኳን','እግር','ከመሆን','ወይንም','ዋና','ዘንድ','የሚከተለው','ያኔ','ይኼው','ገጽ','እነርሱ','ን','ና','ዎች','ይጠበቃል','ብለዋል','ሆ','ሁሉ','አንቀጽ','እንደሆነ','በማይበልጥ','መሰረት','ሁኔታ','ይሆናል','ሆኖ','ከአንድ','በማናቸውም','ወር','ከአምስት','በሆነ','ከዚህ','የሆነ','ሀያ','ሆነ','በኊላ','በአንድ',\n",
    "'የሆኑ','ከአስራ','የሆነውን','መሆኑ','ሌላውን','ከሰባት','ለሌላ','አለበት','ሲል','ይሆናሉ','በሙሉ','አስራ','ቢሆንም','አንዱ','የሌላውን','ከሁለት','የሆኑትን','በሆኑ','ጀምሮ','በመሆን','ባለ','ይህንን','እንዲቆይ','ሌላው','የሚሆነው','በአንዱ','ሲባል','ሳለ','የሆነው','መሆናቸው','በዋና','በማቀድ','ጊዜና','ለዚህ','ሶስተኛ','የነገሩ','ስድስት'\n",
    ",'በሆነው','ይሁን','ከዚሁ','በእነዚህ','ከማናቸውም','ከነበረው','በአንዳንድ','በእያንዳንዱ','ጊዜም','አስከ','የሌሎች','የሚሆኑት','ከሆነው','የነበረውን','ያሉ','ከሌሎች','አንዲት','ለሌሎች','ለሆነው','ሰኣት','ብሎ','ከሰላሳ','የሚሆኑ','ላይም','የሆናል','ከነዚህ','ያህል','ከሆነና','ለሆኑት','እነዚሁ','እንደሆኑ','ስለማናቸውም','ስለዚሁ','ከአንዳንድ','በእነዚሁ','በአምስት','የሆኑበታል','ለነዚህ','ለማንኛውም','አንደኛ','ይኸኛው','ከርሱ','መሆኑን','ለዚያው','ለዚሁ','ለእነርሱም','እዚሁ','ሐ','ረ','ሸ','አምስት','ከሶስት','በተለይም','በሌላ','ሺህ','ማናቸውንም','ከአስር','የማይበልጥ','እንዲሁም','ይህን','የዚህ','ማናቸውም','ከስድስት','መቶ','ያለ','አንድን','ያላቸውን','ሊሆን','ሶስት'\n",
    ",'ካልሆነ','ቢያንስ','ቢሆን','እነዚህን','አንዱን','ይሄ','ሁለት','ወይዘሮ','ተብሎ','ሳይሆን','እንደሆነና','ከብር','ሆኖም','የነበሩ',\n",
    "'የሌላ','ያላቸው','ይህንኑ','ሆነው','በስተቀር','ስም','እንደገና','የማያንስ','እጅግ','እንዲሆን','እንኳ','ከሀያ','ከሀምሳ','ይኸው','ለአንድ','የሚችለውን','በሚገባ','ይህም','እንዲሆኑ','ከሌላ','ለሆነ','በሌሎች','አንደሆነ','እንዲህ',\n",
    "'በነዚሁ','በእንደዚህ','ስምንት','ሲሆንና','ነዉ','ምንጊዜም','ለማናቸውም','የአንድ','እነዚህኑ','ሲሆኑ','በሁለቱም','እንደነዚህ','የሆኑት','የማናቸውም','ይህንንም','የአንድን','በሙሉም','በነዚህ','የዚሁ','ለእያንዳንዱ','ስለሆነ','መሆናቸውን','ማንኛውንም','ሁለቱ','እንጂ','ከስምንት','ሁለቱንም','በሁለት','በእስር','በሚል','ቁጥር','ባሉ','ከመቶ','እነዚህም','ሲኖር','ሰላሳ','ለሆኑ','ሰባት','እነደሆነ','ይህችው','ከእነዚህ','ከእነዚሁ','የአንቀጹ','ወይ','የሆነችን']\n",
    "\n",
    "\n",
    "def stopWordRemoval(tokens):\n",
    "    token_new=[]\n",
    "    for x in tokens:\n",
    "        if x not in stop_words:\n",
    "            token_new.append(x)\n",
    "    return token_new\n",
    "\n",
    "clean_doc=[]\n",
    "for x in my_doc:\n",
    "    clean_doc.append(stopWordRemoval(x))\n",
    "#     print(stopWordRemoval(x))\n",
    "\n",
    "# cast tweets to numpy array\n",
    "docs = clean_doc\n",
    "#docs = processed_lemma\n",
    "# create dictionary of all words in all documents\n",
    "# initialize a Dictionary\n",
    "dictionary = gensim.corpora.Dictionary(docs)\n",
    "\n",
    "# filter extreme cases out of dictionary\n",
    "#dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
    "\n",
    "# create variable containing length of dictionary/vocab\n",
    "vocab_length = len(dictionary)\n",
    "\n",
    "# create BOW dictionary\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "\n",
    "# define function to get words in topics\n",
    "def get_topics_lists(model, top_clusters, n_words):\n",
    "    '''\n",
    "    Gets lists of words in topics as a list of lists.\n",
    "    \n",
    "    model: gsdmm instance\n",
    "    top_clusters:  numpy array containing indices of top_clusters\n",
    "    n_words: top n number of words to include\n",
    "    \n",
    "    '''\n",
    "    # create empty list to contain topics\n",
    "    topics = []\n",
    "    \n",
    "    # iterate over top n clusters\n",
    "    for cluster in top_clusters:\n",
    "        #create sorted dictionary of word distributions\n",
    "        sorted_dict = sorted(model.cluster_word_distribution[cluster].items(), key=lambda k: k[1], reverse=True)[:n_words]\n",
    "         \n",
    "        #create empty list to contain words\n",
    "        topic = []\n",
    "        \n",
    "        #iterate over top n words in topic\n",
    "        for k,v in sorted_dict:\n",
    "            #append words to topic list\n",
    "            topic.append(k)\n",
    "            \n",
    "        #append topics to topics list    \n",
    "        topics.append(topic)\n",
    "    \n",
    "    return topics\n",
    "result = {}\n",
    "for k_ in range(2,51):\n",
    "    for alpha_ in range(0, 101, 1):\n",
    "        for beta_ in range(0,101, 1):\n",
    "            gsdmm = MovieGroupProcess(K=k_, alpha=alpha_/100, beta=beta_/100, n_iters=9)\n",
    "            gsdmm.fit(docs, vocab_length)\n",
    "            # print number of documents per topic\n",
    "            doc_count = np.array(gsdmm.cluster_doc_count)\n",
    "            # print('Number of documents per topic :', doc_count)\n",
    "            \n",
    "            # Topics sorted by the number of document they are allocated to\n",
    "#             print(doc_count)\n",
    "            top_index = doc_count.argsort()[-15:][::-1]\n",
    "            # get topics to feed to coherence model\n",
    "            topics = get_topics_lists(gsdmm, top_index, 50) \n",
    "\n",
    "            # evaluate model using Topic Coherence score\n",
    "            cm_gsdmm = CoherenceModel(topics=topics, \n",
    "                                      model=gsdmm,\n",
    "                                      dictionary=dictionary, \n",
    "                                      corpus=bow_corpus, \n",
    "                                      texts=docs, \n",
    "                                      coherence='c_v')\n",
    "\n",
    "            # get coherence value\n",
    "            coherence_gsdmm = cm_gsdmm.get_coherence()  \n",
    "            if coherence_gsdmm >= 0.4:\n",
    "                print((k_,alpha_,beta_))\n",
    "            result[(k_, alpha_,beta_)] = coherence_gsdmm\n",
    "\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
