{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d52e5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import hm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1332f5",
   "metadata": {},
   "source": [
    "df=pd.read_csv('processedDEV.csv')\n",
    "df=df.drop(['Unnamed: 0'], axis = 1)\n",
    "my_list=df.to_numpy().tolist()\n",
    "len(my_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51243d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#my_doc=pd.read_csv('C:/Users/Kois/LDA/unLemmaTEST.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563a6a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#my_doc=pd.read_csv('C:/Users/Kois/unLemmatizedCorpus.csv') #unLemmatized tokens\n",
    "my_doc=pd.read_csv('C:/Users/Kois/lemmatizedCorpus.csv') #Lemmatized tokens\n",
    "my_doc=my_doc.drop(['Unnamed: 0.1'], axis = 1)\n",
    "my_doc=my_doc.drop(['Unnamed: 0'], axis = 1)\n",
    "my_doc=my_doc.to_numpy().tolist()\n",
    "len(my_doc)\n",
    "my_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a34dde3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "new_list=[]\n",
    "temp_list=[]\n",
    "for x in my_doc:\n",
    "    for item in x:\n",
    "        if str(item) != 'nan':\n",
    "            temp_list.append(item)\n",
    "    new_list.append(temp_list)\n",
    "    temp_list=[]\n",
    "my_doc=new_list\n",
    "len(my_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fd646b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in my_doc:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df362b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words=['እኔ','የእኔ','እኔራሴ','እኛ','የእኛ','እኛራሳችን','አንቺ','ነህ','አላችሁ','እርስዎ','ትፈልጋለህ','ያንተ','ራስህን','እራሳችሁ','እሱ','የእሱ','ራሱ','እሷ','እሷናት','የእሷ','እራሷ','ነው','እነሱ','እነሱን','የእነሱ','ራሳቸው','ምንድን','የትኛው','ማን','ይህ','የሚልነው','ያ','እነዚህ','እነዚያ','ነኝ','ናቸው','ነበር','ነበሩ','ሁን','ቆይቷል','መሆን','አላቸው','አለው','ነበረው','ያለው','መስራት','ያደርጋል','አደረገ','ማድረግ','ሀ','አንድ','የ','እና','ከሆነ','ወይም'\n",
    ",'ምክንያቱም','እንደ','እስከ','እያለ','በ','ለ','ጋር','ስለ','ላይ','መካከል','ወደ','በኩል','ወቅት','ከዚህበፊት','በኋላ','ከላይ','ከታች','ከ','ወደላይ','ታች','ውስጥ','ውጭ','በላይ','እንደገና','ተጨማሪ','ከዚያ','አንድጊዜ','እዚህ','እዚያ','መቼ','የት','ለምን','እንዴት','ሁሉም','ማንኛውም','ሁለቱም','እያንዳንዳቸው','ጥቂቶች','በጣም','ሌላ','አንዳንድ','እንደዚህ','ብቻ','የራሱ','ተመሳሳይ','ስለዚህ','ይልቅ','እንዲሁ','ት','ይችላል','ይገባል','ይገባኛል','አሁን','መ','ም','ኦ','ዳግም','መሆን','ሁለ','ሁለም','ሕዝብ','ሀሙስ','ለመሆኑ','ለምንድን','ሌሎች','መጽሀፍ','ማክሰኞ','ምን','ሰኞ','ሰው','ሲሆን','ስንት','ረቡእ','ቅዳሜ','በዚህ','ብላ','ነገር','አለ','አርብ','አንተ','አንዳንድ','ኢትዮጵያ','እሁድ','እናንተ','እንኳን','እግር','ከመሆን','ወይንም','ዋና','ዘንድ','የሚከተለው','ያኔ','ይኼው','ገጽ','እነርሱ','ን','ና','ዎች','ይጠበቃል','ብለዋል','ሆ','ሁሉ','አንቀጽ','እንደሆነ','በማይበልጥ','መሰረት','ሁኔታ','ይሆናል','ሆኖ','ከአንድ','በማናቸውም','ወር','ከአምስት','በሆነ','ከዚህ','የሆነ','ሀያ','ሆነ','በኊላ','በአንድ',\n",
    "'የሆኑ','ከአስራ','የሆነውን','መሆኑ','ሌላውን','ከሰባት','ለሌላ','አለበት','ሲል','ይሆናሉ','በሙሉ','አስራ','ቢሆንም','አንዱ','የሌላውን','ከሁለት','የሆኑትን','በሆኑ','ጀምሮ','በመሆን','ባለ','ይህንን','እንዲቆይ','ሌላው','የሚሆነው','በአንዱ','ሲባል','ሳለ','የሆነው','መሆናቸው','በዋና','በማቀድ','ጊዜና','ለዚህ','ሶስተኛ','የነገሩ','ስድስት'\n",
    ",'በሆነው','ይሁን','ከዚሁ','በእነዚህ','ከማናቸውም','ከነበረው','በአንዳንድ','በእያንዳንዱ','ጊዜም','አስከ','የሌሎች','የሚሆኑት','ከሆነው','የነበረውን','ያሉ','ከሌሎች','አንዲት','ለሌሎች','ለሆነው','ሰኣት','ብሎ','ከሰላሳ','የሚሆኑ','ላይም','የሆናል','ከነዚህ','ያህል','ከሆነና','ለሆኑት','እነዚሁ','እንደሆኑ','ስለማናቸውም','ስለዚሁ','ከአንዳንድ','በእነዚሁ','በአምስት','የሆኑበታል','ለነዚህ','ለማንኛውም','አንደኛ','ይኸኛው','ከርሱ','መሆኑን','ለዚያው','ለዚሁ','ለእነርሱም','እዚሁ','ሐ','ረ','ሸ','አምስት','ከሶስት','በተለይም','በሌላ','ሺህ','ማናቸውንም','ከአስር','የማይበልጥ','እንዲሁም','ይህን','የዚህ','ማናቸውም','ከስድስት','መቶ','ያለ','አንድን','ያላቸውን','ሊሆን','ሶስት'\n",
    ",'ካልሆነ','ቢያንስ','ቢሆን','እነዚህን','አንዱን','ይሄ','ሁለት','ወይዘሮ','ተብሎ','ሳይሆን','እንደሆነና','ከብር','ሆኖም','የነበሩ',\n",
    "'የሌላ','ያላቸው','ይህንኑ','ሆነው','በስተቀር','ስም','እንደገና','የማያንስ','እጅግ','እንዲሆን','እንኳ','ከሀያ','ከሀምሳ','ይኸው','ለአንድ','የሚችለውን','በሚገባ','ይህም','እንዲሆኑ','ከሌላ','ለሆነ','በሌሎች','አንደሆነ','እንዲህ',\n",
    "'በነዚሁ','በእንደዚህ','ስምንት','ሲሆንና','ነዉ','ምንጊዜም','ለማናቸውም','የአንድ','እነዚህኑ','ሲሆኑ','በሁለቱም','እንደነዚህ','የሆኑት','የማናቸውም','ይህንንም','የአንድን','በሙሉም','በነዚህ','የዚሁ','ለእያንዳንዱ','ስለሆነ','መሆናቸውን','ማንኛውንም','ሁለቱ','እንጂ','ከስምንት','ሁለቱንም','በሁለት','በእስር','በሚል','ቁጥር','ባሉ','ከመቶ','እነዚህም','ሲኖር','ሰላሳ','ለሆኑ','ሰባት','እነደሆነ','ይህችው','ከእነዚህ','ከእነዚሁ','የአንቀጹ','ወይ','የሆነችን']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a868593d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopWordRemoval(tokens):\n",
    "    token_new=[]\n",
    "    for x in tokens:\n",
    "        if x not in stop_words:\n",
    "            token_new.append(x)\n",
    "    return token_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107847b1",
   "metadata": {},
   "source": [
    "processed_docs=[]\n",
    "for item in my_doc:\n",
    "    processed_docs.append(stopWordRemoval(item))\n",
    "#processed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63eaa194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create N-grams\n",
    "def make_n_grams(texts):\n",
    "    bigram = gensim.models.Phrases(texts, min_count=3, threshold=100)  # higher threshold fewer phrases.\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    trigram = gensim.models.Phrases(bigram[texts], threshold=100)\n",
    "    trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "    bigrams_text = [bigram_mod[doc] for doc in texts]\n",
    "    trigrams_text =  [trigram_mod[bigram_mod[doc]] for doc in bigrams_text]\n",
    "    return trigrams_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da03b7f4",
   "metadata": {},
   "source": [
    "def lemmatizer(tokenList):\n",
    "    lemmatized_list=[]\n",
    "    for x in tokenList:\n",
    "        try:\n",
    "            y=hm.anal('amh',x,um=True)\n",
    "            if(y[0]['lemma'].isalpha()):\n",
    "                lemmatized_list.append(x)\n",
    "            else:\n",
    "                b=y[0]['lemma'].split(\"|\")\n",
    "                lemmatized_list.append(b[0])\n",
    "        except:\n",
    "            print('next')\n",
    "            lemmatized_list.append(x)\n",
    "    return lemmatized_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882a3c75",
   "metadata": {},
   "source": [
    "lemmas=[]\n",
    "for items in my_doc:\n",
    "    lemmas.append(lemmatizer(items))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4cd3bb",
   "metadata": {},
   "source": [
    "processed_lemma=[]\n",
    "for x in lemmas:\n",
    "    processed_lemma.append(stopWordRemoval(x))\n",
    "    print(stopWordRemoval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b5194e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_doc=[]\n",
    "for x in my_doc:\n",
    "    clean_doc.append(stopWordRemoval(x))\n",
    "    print(stopWordRemoval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a0a81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_lemma=make_n_grams(my_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947e0761",
   "metadata": {},
   "source": [
    "import hm\n",
    "listOfNouns=[]\n",
    "for x in processed_docs:\n",
    "    for l in x:\n",
    "        y=hm.anal('amh',l,um=True)\n",
    "        try:\n",
    "            if y[1]['POS']=='n':\n",
    "                listOfNouns.append(l)\n",
    "        except:\n",
    "            if y==[]:\n",
    "                print()\n",
    "            elif y[0]['POS']=='n':\n",
    "                listOfNouns.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7ff5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "listOfNouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999f686b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gsdmm import MovieGroupProcess\n",
    "\n",
    "# cast tweets to numpy array\n",
    "docs = clean_doc\n",
    "#docs = processed_lemma\n",
    "# create dictionary of all words in all documents\n",
    "# initialize a Dictionary\n",
    "dictionary = gensim.corpora.Dictionary(docs)\n",
    "\n",
    "# filter extreme cases out of dictionary\n",
    "#dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
    "\n",
    "# create variable containing length of dictionary/vocab\n",
    "vocab_length = len(dictionary)\n",
    "\n",
    "# create BOW dictionary\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "\n",
    "# initialize GSDMM\n",
    "gsdmm = MovieGroupProcess(K=5, alpha=0.4, beta=0.1, n_iters=1)\n",
    "\n",
    "# fit GSDMM model\n",
    "y = gsdmm.fit(docs, vocab_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b50f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07efade4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print number of documents per topic\n",
    "doc_count = np.array(gsdmm.cluster_doc_count)\n",
    "print('Number of documents per topic :', doc_count)\n",
    "\n",
    "# Topics sorted by the number of document they are allocated to\n",
    "top_index = doc_count.argsort()[-15:][::-1]\n",
    "print('Most important clusters (by number of docs inside):', top_index)\n",
    "\n",
    "# define function to get top words per topic\n",
    "def top_words(cluster_word_distribution, top_cluster, values):\n",
    "    for cluster in top_cluster:\n",
    "        sort_dicts = sorted(cluster_word_distribution[cluster].items(), key=lambda k: k[1], reverse=True)[:values]\n",
    "        print(\"\\nCluster %s : %s\"%(cluster, sort_dicts))\n",
    "\n",
    "# get top words in topics\n",
    "top_words(gsdmm.cluster_word_distribution, top_index, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e46088",
   "metadata": {},
   "source": [
    "conda install -c conda-forge wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd121659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library from gensim  \n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# define function to get words in topics\n",
    "def get_topics_lists(model, top_clusters, n_words):\n",
    "    '''\n",
    "    Gets lists of words in topics as a list of lists.\n",
    "    \n",
    "    model: gsdmm instance\n",
    "    top_clusters:  numpy array containing indices of top_clusters\n",
    "    n_words: top n number of words to include\n",
    "    \n",
    "    '''\n",
    "    # create empty list to contain topics\n",
    "    topics = []\n",
    "    \n",
    "    # iterate over top n clusters\n",
    "    for cluster in top_clusters:\n",
    "        #create sorted dictionary of word distributions\n",
    "        sorted_dict = sorted(model.cluster_word_distribution[cluster].items(), key=lambda k: k[1], reverse=True)[:n_words]\n",
    "         \n",
    "        #create empty list to contain words\n",
    "        topic = []\n",
    "        \n",
    "        #iterate over top n words in topic\n",
    "        for k,v in sorted_dict:\n",
    "            #append words to topic list\n",
    "            topic.append(k)\n",
    "            \n",
    "        #append topics to topics list    \n",
    "        topics.append(topic)\n",
    "    \n",
    "    return topics\n",
    "\n",
    "# get topics to feed to coherence model\n",
    "topics = get_topics_lists(gsdmm, top_index, 50) \n",
    "\n",
    "# evaluate model using Topic Coherence score\n",
    "cm_gsdmm = CoherenceModel(topics=topics, \n",
    "                          model=gsdmm,\n",
    "                          dictionary=dictionary, \n",
    "                          corpus=bow_corpus, \n",
    "                          texts=docs, \n",
    "                          coherence='c_v')\n",
    "\n",
    "# get coherence value\n",
    "coherence_gsdmm = cm_gsdmm.get_coherence()  \n",
    "\n",
    "print(coherence_gsdmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cac5e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import hm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import library from gensim  \n",
    "from gensim.models import CoherenceModel\n",
    "from gsdmm import MovieGroupProcess\n",
    "\n",
    "my_doc=pd.read_csv('C:/Users/Kois/lemmatizedCorpus.csv') #Lemmatized tokens\n",
    "my_doc=my_doc.drop(['Unnamed: 0.1'], axis = 1)\n",
    "my_doc=my_doc.drop(['Unnamed: 0'], axis = 1)\n",
    "my_doc=my_doc.to_numpy().tolist()\n",
    "len(my_doc)\n",
    "my_doc\n",
    "\n",
    "new_list=[]\n",
    "temp_list=[]\n",
    "for x in my_doc:\n",
    "    for item in x:\n",
    "        if str(item) != 'nan':\n",
    "            temp_list.append(item)\n",
    "    new_list.append(temp_list)\n",
    "    temp_list=[]\n",
    "my_doc=new_list\n",
    "len(my_doc)\n",
    "\n",
    "stop_words=['እኔ','የእኔ','እኔራሴ','እኛ','የእኛ','እኛራሳችን','አንቺ','ነህ','አላችሁ','እርስዎ','ትፈልጋለህ','ያንተ','ራስህን','እራሳችሁ','እሱ','የእሱ','ራሱ','እሷ','እሷናት','የእሷ','እራሷ','ነው','እነሱ','እነሱን','የእነሱ','ራሳቸው','ምንድን','የትኛው','ማን','ይህ','የሚልነው','ያ','እነዚህ','እነዚያ','ነኝ','ናቸው','ነበር','ነበሩ','ሁን','ቆይቷል','መሆን','አላቸው','አለው','ነበረው','ያለው','መስራት','ያደርጋል','አደረገ','ማድረግ','ሀ','አንድ','የ','እና','ከሆነ','ወይም'\n",
    ",'ምክንያቱም','እንደ','እስከ','እያለ','በ','ለ','ጋር','ስለ','ላይ','መካከል','ወደ','በኩል','ወቅት','ከዚህበፊት','በኋላ','ከላይ','ከታች','ከ','ወደላይ','ታች','ውስጥ','ውጭ','በላይ','እንደገና','ተጨማሪ','ከዚያ','አንድጊዜ','እዚህ','እዚያ','መቼ','የት','ለምን','እንዴት','ሁሉም','ማንኛውም','ሁለቱም','እያንዳንዳቸው','ጥቂቶች','በጣም','ሌላ','አንዳንድ','እንደዚህ','ብቻ','የራሱ','ተመሳሳይ','ስለዚህ','ይልቅ','እንዲሁ','ት','ይችላል','ይገባል','ይገባኛል','አሁን','መ','ም','ኦ','ዳግም','መሆን','ሁለ','ሁለም','ሕዝብ','ሀሙስ','ለመሆኑ','ለምንድን','ሌሎች','መጽሀፍ','ማክሰኞ','ምን','ሰኞ','ሰው','ሲሆን','ስንት','ረቡእ','ቅዳሜ','በዚህ','ብላ','ነገር','አለ','አርብ','አንተ','አንዳንድ','ኢትዮጵያ','እሁድ','እናንተ','እንኳን','እግር','ከመሆን','ወይንም','ዋና','ዘንድ','የሚከተለው','ያኔ','ይኼው','ገጽ','እነርሱ','ን','ና','ዎች','ይጠበቃል','ብለዋል','ሆ','ሁሉ','አንቀጽ','እንደሆነ','በማይበልጥ','መሰረት','ሁኔታ','ይሆናል','ሆኖ','ከአንድ','በማናቸውም','ወር','ከአምስት','በሆነ','ከዚህ','የሆነ','ሀያ','ሆነ','በኊላ','በአንድ',\n",
    "'የሆኑ','ከአስራ','የሆነውን','መሆኑ','ሌላውን','ከሰባት','ለሌላ','አለበት','ሲል','ይሆናሉ','በሙሉ','አስራ','ቢሆንም','አንዱ','የሌላውን','ከሁለት','የሆኑትን','በሆኑ','ጀምሮ','በመሆን','ባለ','ይህንን','እንዲቆይ','ሌላው','የሚሆነው','በአንዱ','ሲባል','ሳለ','የሆነው','መሆናቸው','በዋና','በማቀድ','ጊዜና','ለዚህ','ሶስተኛ','የነገሩ','ስድስት'\n",
    ",'በሆነው','ይሁን','ከዚሁ','በእነዚህ','ከማናቸውም','ከነበረው','በአንዳንድ','በእያንዳንዱ','ጊዜም','አስከ','የሌሎች','የሚሆኑት','ከሆነው','የነበረውን','ያሉ','ከሌሎች','አንዲት','ለሌሎች','ለሆነው','ሰኣት','ብሎ','ከሰላሳ','የሚሆኑ','ላይም','የሆናል','ከነዚህ','ያህል','ከሆነና','ለሆኑት','እነዚሁ','እንደሆኑ','ስለማናቸውም','ስለዚሁ','ከአንዳንድ','በእነዚሁ','በአምስት','የሆኑበታል','ለነዚህ','ለማንኛውም','አንደኛ','ይኸኛው','ከርሱ','መሆኑን','ለዚያው','ለዚሁ','ለእነርሱም','እዚሁ','ሐ','ረ','ሸ','አምስት','ከሶስት','በተለይም','በሌላ','ሺህ','ማናቸውንም','ከአስር','የማይበልጥ','እንዲሁም','ይህን','የዚህ','ማናቸውም','ከስድስት','መቶ','ያለ','አንድን','ያላቸውን','ሊሆን','ሶስት'\n",
    ",'ካልሆነ','ቢያንስ','ቢሆን','እነዚህን','አንዱን','ይሄ','ሁለት','ወይዘሮ','ተብሎ','ሳይሆን','እንደሆነና','ከብር','ሆኖም','የነበሩ',\n",
    "'የሌላ','ያላቸው','ይህንኑ','ሆነው','በስተቀር','ስም','እንደገና','የማያንስ','እጅግ','እንዲሆን','እንኳ','ከሀያ','ከሀምሳ','ይኸው','ለአንድ','የሚችለውን','በሚገባ','ይህም','እንዲሆኑ','ከሌላ','ለሆነ','በሌሎች','አንደሆነ','እንዲህ',\n",
    "'በነዚሁ','በእንደዚህ','ስምንት','ሲሆንና','ነዉ','ምንጊዜም','ለማናቸውም','የአንድ','እነዚህኑ','ሲሆኑ','በሁለቱም','እንደነዚህ','የሆኑት','የማናቸውም','ይህንንም','የአንድን','በሙሉም','በነዚህ','የዚሁ','ለእያንዳንዱ','ስለሆነ','መሆናቸውን','ማንኛውንም','ሁለቱ','እንጂ','ከስምንት','ሁለቱንም','በሁለት','በእስር','በሚል','ቁጥር','ባሉ','ከመቶ','እነዚህም','ሲኖር','ሰላሳ','ለሆኑ','ሰባት','እነደሆነ','ይህችው','ከእነዚህ','ከእነዚሁ','የአንቀጹ','ወይ','የሆነችን']\n",
    "\n",
    "\n",
    "def stopWordRemoval(tokens):\n",
    "    token_new=[]\n",
    "    for x in tokens:\n",
    "        if x not in stop_words:\n",
    "            token_new.append(x)\n",
    "    return token_new\n",
    "\n",
    "clean_doc=[]\n",
    "for x in my_doc:\n",
    "    clean_doc.append(stopWordRemoval(x))\n",
    "#     print(stopWordRemoval(x))\n",
    "\n",
    "# cast tweets to numpy array\n",
    "docs = clean_doc\n",
    "#docs = processed_lemma\n",
    "# create dictionary of all words in all documents\n",
    "# initialize a Dictionary\n",
    "dictionary = gensim.corpora.Dictionary(docs)\n",
    "\n",
    "# filter extreme cases out of dictionary\n",
    "#dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
    "\n",
    "# create variable containing length of dictionary/vocab\n",
    "vocab_length = len(dictionary)\n",
    "\n",
    "# create BOW dictionary\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "\n",
    "# define function to get words in topics\n",
    "def get_topics_lists(model, top_clusters, n_words):\n",
    "    '''\n",
    "    Gets lists of words in topics as a list of lists.\n",
    "    \n",
    "    model: gsdmm instance\n",
    "    top_clusters:  numpy array containing indices of top_clusters\n",
    "    n_words: top n number of words to include\n",
    "    \n",
    "    '''\n",
    "    # create empty list to contain topics\n",
    "    topics = []\n",
    "    \n",
    "    # iterate over top n clusters\n",
    "    for cluster in top_clusters:\n",
    "        #create sorted dictionary of word distributions\n",
    "        sorted_dict = sorted(model.cluster_word_distribution[cluster].items(), key=lambda k: k[1], reverse=True)[:n_words]\n",
    "         \n",
    "        #create empty list to contain words\n",
    "        topic = []\n",
    "        \n",
    "        #iterate over top n words in topic\n",
    "        for k,v in sorted_dict:\n",
    "            #append words to topic list\n",
    "            topic.append(k)\n",
    "            \n",
    "        #append topics to topics list    \n",
    "        topics.append(topic)\n",
    "    \n",
    "    return topics\n",
    "result = {}\n",
    "for k_ in range(2,11):\n",
    "    for alpha_ in range(0, 11, 1):\n",
    "        for beta_ in range(0,11, 1):\n",
    "            gsdmm = MovieGroupProcess(K=k_, alpha=alpha_/10, beta=beta_/10, n_iters=9)\n",
    "            gsdmm.fit(docs, vocab_length)\n",
    "            # print number of documents per topic\n",
    "            doc_count = np.array(gsdmm.cluster_doc_count)\n",
    "            # print('Number of documents per topic :', doc_count)\n",
    "            \n",
    "            # Topics sorted by the number of document they are allocated to\n",
    "#             print(doc_count)\n",
    "            top_index = doc_count.argsort()[-15:][::-1]\n",
    "            # get topics to feed to coherence model\n",
    "            topics = get_topics_lists(gsdmm, top_index, 50) \n",
    "\n",
    "            # evaluate model using Topic Coherence score\n",
    "            cm_gsdmm = CoherenceModel(topics=topics, \n",
    "                                      model=gsdmm,\n",
    "                                      dictionary=dictionary, \n",
    "                                      corpus=bow_corpus, \n",
    "                                      texts=docs, \n",
    "                                      coherence='c_v')\n",
    "\n",
    "            # get coherence value\n",
    "            coherence_gsdmm = cm_gsdmm.get_coherence()  \n",
    "            if coherence_gsdmm >= 0.4:\n",
    "                print((k_,alpha_/10,beta_/10))\n",
    "            result[(k_, alpha_/10,beta_/10)] = coherence_gsdmm\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee418492",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, doc_term_matrix, start, stop, step):\n",
    "    \"\"\"\n",
    "    Input   : dictionary : Gensim dictionary\n",
    "              corpus : Gensim corpus\n",
    "              texts : List of input texts\n",
    "              stop : Max num of topics\n",
    "    purpose : Compute c_v coherence for various number of topics\n",
    "    Output  : model_list : List of LSA topic models\n",
    "              coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, stop, step):\n",
    "        gsdmm = MovieGroupProcess(K=num_topics, alpha=1, beta=1, n_iters=9)\n",
    "        # fit GSDMM model\n",
    "        y = gsdmm.fit(docs, vocab_length)\n",
    "        topics = get_topics_lists(gsdmm, top_index, 50) \n",
    "        cm_gsdmm = CoherenceModel(model=gsdmm,topics=topics, \n",
    "                          dictionary=dictionary, \n",
    "                          corpus=bow_corpus, \n",
    "                          texts=docs, \n",
    "                          coherence='c_v')\n",
    "        coherence_values.append(cm_gsdmm.get_coherence())\n",
    "    return coherence_values\n",
    "    #return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebb86e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(doc_clean,start, stop, step):\n",
    "    coherence_values = compute_coherence_values(dictionary, bow_corpus, start,stop, step)\n",
    "    # Show graph\n",
    "    x = range(start, stop, step)\n",
    "    plt.plot(x, coherence_values)\n",
    "    plt.xlabel(\"Number of Topics\")\n",
    "    plt.ylabel(\"Coherence score\")\n",
    "    plt.legend((\"coherence_values\"), loc='best')\n",
    "    plt.show()\n",
    "\n",
    "start,stop,step=2,10,1\n",
    "plot_graph(bow_corpus,start,stop,step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d8e787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library from gensim  \n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# instantiate topic coherence model\n",
    "cm = CoherenceModel(model=lda_model, corpus=bow_corpus, texts=docs, coherence='c_v')\n",
    "\n",
    "# get topic coherence score\n",
    "coherence_lda = cm.get_coherence() \n",
    "print(coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d81e07",
   "metadata": {},
   "source": [
    "lemma_df = pd.DataFrame(processed_lemma) \n",
    "lemma_df.to_csv('LemmaTEST.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72602fc8",
   "metadata": {},
   "source": [
    "pip install git+https://github.com/atefm/pDMM.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a2929d",
   "metadata": {},
   "source": [
    "pip install git+https://github.com/rwalk/gsdmm.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0808ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('C:/Users/Kois/Desktop/thesis/dataset/tweets_long/outTRAIN.csv')\n",
    "data=data[data['tweet']!='lost']\n",
    "data.tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a7be68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_topics_dataframe(data_text=data.tweet,  mgp=gsdmm, threshold=0.3, topic_dict=topics):\n",
    "    result = pd.DataFrame(columns=['Text', 'Topic'])\n",
    "    for i, text in enumerate(data_text):\n",
    "        result.at[i, 'Text'] = text\n",
    "        prob = mgp.choose_best_label(clean_doc[i])\n",
    "        if prob[1] >= threshold:\n",
    "            result.at[i, 'Topic'] =prob[0]\n",
    "            #print(prob[0])\n",
    "            #result.at[i, 'Topic'] = topic_dict[prob[0]]\n",
    "        else:\n",
    "            result.at[i, 'Topic'] = 'Other'\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b55ae83",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = create_topics_dataframe(data_text=data.tweet,  mgp=gsdmm, threshold=0.3, topic_dict=top_index)\n",
    "result.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b849a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "values=20\n",
    "word_cluster=[]\n",
    "for cluster in top_index:\n",
    "    sort_dicts = sorted(gsdmm.cluster_word_distribution[cluster].items(), key=lambda k: k[1], reverse=True)[:values]\n",
    "    word_cluster.append(sort_dicts[1][0])\n",
    "    print(word_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf209f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=hm.anal('amh','ሰበረችው',um=True)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357e0099",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc8dfb8f",
   "metadata": {},
   "source": [
    "# importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "\n",
    "# cast tweets to numpy array\n",
    "docs = processed_docs\n",
    "\n",
    "# create dictionary of all words in all documents\n",
    "dictionary = gensim.corpora.Dictionary(docs)\n",
    "\n",
    "# filter extreme cases out of dictionary\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
    "\n",
    "# create BOW dictionary\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "\n",
    "# create LDA model using preferred hyperparameters\n",
    "lda_model = gensim.models.LdaMulticore(bow_corpus, \n",
    "                                         num_topics=6, \n",
    "                                         id2word=dictionary, \n",
    "                                         passes=4, \n",
    "                                         workers=2,\n",
    "                                         random_state=21)\n",
    "\n",
    "# Save LDA model to disk\n",
    "path_to_model = \"C:\\\\Users\\\\Kois\\\\Desktop\\\\thesis\\\\dataset\\\\tweets_long\\\\model\\\\GSDMM.pkl\"\n",
    "lda_model.save(path_to_model)\n",
    "\n",
    "# for each topic, print words occuring in that topic\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
