{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d52e5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "@@@@ This is HornMorpho, version 4.0.5 @@@@\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import hm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bea56ad",
   "metadata": {},
   "source": [
    "df=pd.read_csv('processedDEV.csv')\n",
    "df=df.drop(['Unnamed: 0'], axis = 1)\n",
    "my_list=df.to_numpy().tolist()\n",
    "len(my_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d51243d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#my_doc=pd.read_csv('C:/Users/Kois/LDA/unLemmaTEST.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "563a6a81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#my_doc=pd.read_csv('C:/Users/Kois/unLemmatizedCorpus.csv') #unLemmatized tokens\n",
    "#my_doc=pd.read_csv('C:/Users/Kois/lemmatizedCorpus.csv') #Lemmatized tokens\n",
    "my_doc=pd.read_csv('C:/Users/Kois/LDA/36TokenizedSentences.csv')\n",
    "#my_doc=my_doc.drop(['Unnamed: 0.1'], axis = 1)\n",
    "my_doc=my_doc.drop(['Unnamed: 0'], axis = 1)\n",
    "my_doc=my_doc.to_numpy().tolist()\n",
    "len(my_doc)\n",
    "#my_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a34dde3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "new_list=[]\n",
    "temp_list=[]\n",
    "for x in my_doc:\n",
    "    for item in x:\n",
    "        if str(item) != 'nan':\n",
    "            temp_list.append(item)\n",
    "    new_list.append(temp_list)\n",
    "    temp_list=[]\n",
    "my_doc=new_list\n",
    "#len(my_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df362b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words=['እኔ', 'የእኔ', 'እኔራሤ', 'እኛ', 'የእኛ', 'እኛራሣችን', 'አንቺ', 'ነህ', 'አላችሁ', 'እርሥዎ', 'ትፈልጋለህ', 'ያንተ', 'ራሥህን', 'እራሣችሁ', 'እሡ', 'የእሡ',\n",
    " 'ራሡ', 'እሷ', 'እሷናት', 'የእሷ', 'እራሷ', 'ነው', 'እነሡ', 'እነሡን', 'የእነሡ', 'ራሣቸው', 'ምንድን', 'የትኛው', 'ማን', 'ይህ', 'የሚልነው', 'ያ', 'እነዚህ', 'እነዚያ', 'ነኝ', 'ናቸው', 'ነበር', 'ነበሩ', 'ሁን', 'ቆይቷል', 'መሆን',\n",
    " 'አላቸው', 'አለው', 'ነበረው', 'ያለው', 'መሥራት', 'ያደርጋል', 'አደረገ', 'ማድረግ', 'ሀ', 'አንድ', 'የ', 'እና', 'ከሆነ', 'ወይም', 'ምክንያቱም','ነው', 'ነበር',\n",
    " 'እንደ', 'እሥከ', 'እያለ', 'በ', 'ለ', 'ጋር', 'ሥለ', 'ላይ', 'መካከል', 'ወደ', 'በኩል', 'ወቅት', 'ከዚህበፊት', 'በኋላ', 'ከላይ', 'ከታች', 'ከ', 'ወደላይ', 'ታች', 'ውሥጥ', 'ውጭ', 'በላይ',\n",
    " 'እንደገና', 'ተጨማሪ', 'ከዚያ', 'አንድጊዜ', 'እዚህ', 'እዚያ', 'መቼ', 'የት', 'ለምን', 'እንዴት', 'ሁሉም', 'ማንኛውም', 'ሁለቱም', 'እያንዳንዳቸው', 'ጥቂቶች', 'በጣም', 'ሌላ', 'አንዳንድ',\n",
    " 'እንደዚህ', 'ብቻ', 'የራሡ', 'ተመሣሣይ', 'ሥለዚህ', 'ይልቅ', 'እንዲሁ', 'ት', 'ይችላል', 'ይገባል', 'ይገባኛል', 'አሁን', 'መ', 'ም', 'ኦ', 'ዳግም',\n",
    " 'መሆን', 'ሁለ', 'ሁለም', 'ህዝብ', 'ሀሙሥ', 'ለመሆኑ', 'ለምንድን', 'ሌሎች', 'መጽሀፍ', 'ማክሠኞ', 'ምን', 'ሠኞ', 'ሠው', 'ሢሆን', 'ሥንት', 'ረቡእ',\n",
    " 'ቅዳሜ', 'በዚህ', 'ብላ', 'ነገር', 'አለ', 'አርብ', 'አንተ', 'አንዳንድ', 'ኢትዮጵያ', 'እሁድ', 'እናንተ', 'እንኳን', 'እግር', 'ከመሆን', 'ወይንም', 'ዋና', 'ዘንድ', 'የሚከተለው', 'ያኔ', 'ይኼው', 'ገጽ', 'እነርሡ',\n",
    " 'ን', 'ና', 'ዎች','ኛ', 'ይጠበቃል', 'ብለዋል', 'ሆ', 'ግን','ሁሉ', 'አንቀጽ', 'እንደሆነ', 'በማይበልጥ', 'መሠረት', 'ሁኔታ', 'ይሆናል', 'ሆኖ', 'ከአንድ', 'በማናቸውም',\n",
    " 'ወር', 'ከአምሥት', 'በሆነ', 'ከዚህ', 'የሆነ', 'ሀያ', 'ሆነ', 'በኊላ', 'በአንድ', 'የሆኑ', 'ከአሥራ', 'የሆነውን', 'መሆኑ', 'ሌላውን', 'ከሠባት', 'ለሌላ',\n",
    " 'አለበት', 'ሢል', 'ይሆናሉ', 'በሙሉ', 'አሥራ', 'ቢሆንም', 'አንዱ', 'የሌላውን', 'ከሁለት', 'የሆኑትን', 'በሆኑ', 'ጀምሮ', 'በመሆን', 'ባለ', 'ይህንን', 'እንዲቆይ', 'ሌላው', 'የሚሆነው', 'በአንዱ', 'ሢባል', 'ሣለ', 'የሆነው', 'መሆናቸው', 'በዋና', 'በማቀድ', 'ጊዜና', 'ለዚህ', 'ሦሥተኛ', 'የነገሩ', 'ሥድሥት', 'በሆነው', 'ይሁን', 'ከዚሁ', 'በእነዚህ', 'ከማናቸውም', 'ከነበረው',\n",
    " 'በአንዳንድ', 'በእያንዳንዱ', 'ጊዜም', 'አሥከ', 'የሌሎች', 'የሚሆኑት', 'ከሆነው', 'የነበረውን', 'ያሉ', 'ከሌሎች', 'አንዲት', 'ለሌሎች', 'ለሆነው', 'ሠኣት', 'ብሎ', 'ከሠላሣ',\n",
    " 'የሚሆኑ', 'ላይም', 'የሆናል', 'ከነዚህ', 'ያህል', 'ከሆነና', 'ለሆኑት', 'እነዚሁ', 'እንደሆኑ', 'ሥለማናቸውም', 'ሥለዚሁ', 'ከአንዳንድ',\n",
    " 'በእነዚሁ', 'በአምሥት', 'የሆኑበታል', 'ለነዚህ', 'ለማንኛውም', 'አንደኛ', 'ይኸኛው', 'ከርሡ', 'መሆኑን', 'ለዚያው', 'ለዚሁ', 'ለእነርሡም', 'እዚሁ', 'ሀ', 'ረ', 'ሸ', 'አምሥት',\n",
    " 'ከሦሥት', 'በተለይም', 'በሌላ', 'ሺህ', 'ማናቸውንም', 'ከአሥር', 'የማይበልጥ', 'እንዲሁም', 'ይህን', 'የዚህ', 'ማናቸውም', 'ከሥድሥት', 'መቶ', 'ያለ', 'አንድን',\n",
    " 'ያላቸውን', 'ሊሆን', 'ሦሥት', 'ካልሆነ', 'ቢያንሥ', 'ቢሆን', 'እነዚህን', 'አንዱን', 'ሁለት', 'ወይዘሮ', 'ተብሎ', 'ሣይሆን', 'እንደሆነና', 'ከብር', 'ሆኖም', 'የነበሩ',\n",
    " 'የሌላ', 'ያላቸው', 'ይህንኑ', 'ሆነው', 'በሥተቀር', 'ሥም', 'እንደገና', 'የማያንሥ', 'እጅግ', 'እንዲሆን', 'እንኳ', 'ከሀያ', 'ከሀምሣ', 'ይኸው', 'ለአንድ', 'የሚችለውን',\n",
    " 'በሚገባ', 'ይህም', 'እንዲሆኑ', 'ከሌላ', 'ለሆነ', 'በሌሎች', 'አንደሆነ', 'እንዲህ', 'በነዚሁ', 'በእንደዚህ', 'ሥምንት', 'ሢሆንና', 'ምንጊዜም', 'ለማናቸውም', 'የአንድ', 'እነዚህኑ', 'ሢሆኑ', 'በሁለቱም',\n",
    " 'እንደነዚህ', 'የሆኑት', 'የማናቸውም', 'ይህንንም', 'የአንድን', 'በሙሉም', 'በነዚህ', 'የዚሁ', 'ለእያንዳንዱ', 'ሥለሆነ', 'መሆናቸውን', 'ማንኛውንም', 'ሁለቱ', 'እንጂ',\n",
    " 'ከሥምንት', 'ሁለቱንም', 'በሁለት', 'በእሥር', 'በሚል', 'ቁጥር', 'ባሉ', 'ከመቶ', 'እነዚህም', 'ሢኖር', 'ሠላሣ', 'ለሆኑ', 'ሠባት', 'እነደሆነ', 'ይህችው', 'ከእነዚህ',\n",
    " 'ከእነዚሁ', 'የአንቀጹ', 'ወይ', 'የሆነችን','እኮ','ኧረ','ጋ','እንዴ','ነበረ','ነዉ','ነሀ','ናቸዉ','ድርግ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a868593d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopWordRemoval(tokens):\n",
    "    token_new=[]\n",
    "    for x in tokens:\n",
    "        if x not in stop_words:\n",
    "            token_new.append(x)\n",
    "    return token_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8de4d45",
   "metadata": {},
   "source": [
    "processed_docs=[]\n",
    "for item in my_doc:\n",
    "    processed_docs.append(stopWordRemoval(item))\n",
    "#processed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "63eaa194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create N-grams\n",
    "def make_n_grams(texts):\n",
    "    bigram = gensim.models.Phrases(texts, min_count=3, threshold=100)  # higher threshold fewer phrases.\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    trigram = gensim.models.Phrases(bigram[texts], threshold=100)\n",
    "    trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "    bigrams_text = [bigram_mod[doc] for doc in texts]\n",
    "    trigrams_text =  [trigram_mod[bigram_mod[doc]] for doc in bigrams_text]\n",
    "    return trigrams_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da03b7f4",
   "metadata": {},
   "source": [
    "def lemmatizer(tokenList):\n",
    "    lemmatized_list=[]\n",
    "    for x in tokenList:\n",
    "        try:\n",
    "            y=hm.anal('amh',x,um=True)\n",
    "            if(y[0]['lemma'].isalpha()):\n",
    "                lemmatized_list.append(x)\n",
    "            else:\n",
    "                b=y[0]['lemma'].split(\"|\")\n",
    "                lemmatized_list.append(b[0])\n",
    "        except:\n",
    "            print('next')\n",
    "            lemmatized_list.append(x)\n",
    "    return lemmatized_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882a3c75",
   "metadata": {},
   "source": [
    "lemmas=[]\n",
    "for items in my_doc:\n",
    "    lemmas.append(lemmatizer(items))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4cd3bb",
   "metadata": {},
   "source": [
    "processed_lemma=[]\n",
    "for x in lemmas:\n",
    "    processed_lemma.append(stopWordRemoval(x))\n",
    "    print(stopWordRemoval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12b5194e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['መብት', 'የለም', 'ብልጽግና', 'ፓርቲ', 'መሰረተ']\n",
      "['ኢትዮጲያ', 'መሪ', 'ፓርቲ', 'ብልጽና']\n",
      "['ደረሰ', 'ልጅ']\n",
      "['ምርጥ', 'መሪ', 'ደስ', 'ሀገር', 'ጠራ']\n",
      "['እውቀት', 'ጥግ', 'ሰማ', 'ሰማ', 'ተናገረ', 'ታሪክ', 'አነበበ', 'ራስ', 'ገመተ']\n",
      "['አማራ', 'ክልል', 'ፈላ', 'ልክ', 'ገባ', 'ሙስሊም', 'ጠላላ', 'ሙስሊም', 'ኖረ', 'ኦሮምያ', 'ግንኙነት', 'አሰበ']\n",
      "['በታች', 'ስሜት', 'ማንም', 'አክራሪ', 'ሰከረ', 'መጣ', 'መስጂድ', 'ሲያቃጥልብህ', 'ተናገረ', 'መልሰህ', 'ገባ', 'አይነት', 'አለቀለቀ', 'ምስጋና', 'ፈለገ', 'አይደለም', 'ታገለ', 'አጎነበሰ', 'ቻለ']\n",
      "['አብይ', 'ጀዋር', 'ተጠቀመ', 'አረገ', 'ተጠያቂ', 'አብይ']\n",
      "['አብይ', 'ሁሌም', 'አሸናፊ', 'ሁሌም', 'ተሸላሚ', 'ሀገር', 'ወገን', 'ድንቅ', 'በረከት', 'አበረከተ', 'አለም', 'ተረባረበ', 'ሸለመ', 'መሪ']\n",
      "['ብዙ', 'አወራ', 'አሸናፊ', 'ኑሮ', 'ጌታቸው', 'ረዳ', 'አፍሪካ', 'ቀንድ', 'ቁጥጥር', 'ስር', 'አዋለ']\n",
      "['ህዋህት', 'ነቀርሳ', 'መዥገር', 'ጨርሰ', 'ጠፋ', 'መከራ', 'ቀጠለ']\n",
      "['ተፈጸመ', 'እስክል', 'ቸኮለ', 'አብይ', 'ጀምረ', 'ጨረሰ', 'የለም', 'ድል', 'ጥምር', 'ሀይል']\n",
      "['ሀገር', 'ዘላቂ', 'ሰላም', 'መጣ', 'ፈለገ', 'ተሳካ', 'ጠበቀ']\n",
      "['መልካም', 'እሴትት', 'ማሳደግ', 'ችግር', 'ሰላም', 'ፈታ', 'ዜጋ', 'አብሮ', 'ቆመ']\n",
      "['ቁልጭ', 'መኩሪያ']\n",
      "['መልካም', 'አርጋ', 'ተጫወተ']\n",
      "['ንግግር', 'ጊዜ', 'አሜን', 'ደከመ']\n",
      "['አትሌት', 'ሀይሌ', 'ጠቃሚ', 'ሀገር', 'ሰራ', 'አደነቀ']\n",
      "['ትክክል', 'አዲስ', 'አበባ', 'ቆመ', 'ፓርቲ', 'ባልደራስ', 'እውነተኛ', 'ዲሞክራሲ', 'ድል', 'ዲሞክራሲ', 'ድል']\n",
      "['ይሄን', 'ጉዳይ', 'ክልል', 'አጠቃ']\n",
      "['አሜን', 'በረከት', 'ደረሰ']\n",
      "['ሰላም', 'በዛ']\n",
      "['ተለየ', 'እግዚአብሄር', 'ጠቅላይ', 'ሚኒስትር', 'አረገ']\n",
      "['ስድብ', 'ይቅር', 'ተናገረ']\n",
      "['ሰናይት', 'ነውር', 'ሴት', 'ተማረ', 'ትውልድ', 'ገደለ', 'ተፈጠረ', 'ተሻለ', 'ወለደ', 'ዘር', 'ተካ', 'ልጅ', 'ዘር', 'ተካ', 'ጠፋ', 'ሰራ', 'ወንጀል', 'ፈጣሪ', 'ስራ', 'ሰጠ']\n",
      "['ታፈነ', 'ሴት', 'ተመለከተ']\n",
      "['ጸረ', 'ክርስቲያን', 'ሙስሊም', 'ወከለ', 'ጸረ']\n",
      "['ቻለ', 'ጠላት', 'ሰይጣን', 'ክርስትያን', 'በስመአብ', 'አማተበ', 'ተከላከለ', 'ሙስሊም', 'ረገመ', 'ሰይጣን', 'አላህ', 'ጠበቀ', 'ተከላከለ']\n",
      "['ጨዋ', 'ጥል', 'ረካ', 'ፍቅር', 'ዳብረ', 'በጎ', 'እህት']\n",
      "['ተማሪ', 'ፖለቲካ', 'ጨዋታ', 'ሰለባ', 'መልክ', 'ተቀባይ', 'መንግስት', 'ታገተ', 'ተማሪ', 'ጉዳይ', 'ሰራ', 'ግልጽ', 'መረጃ', 'ሰጠ']\n",
      "['ሞጣ', 'ፈጸመ', 'መስጂድ', 'አቃጠለ', 'ነውር', 'ጤና', 'አወገዘ', 'ቻለ', 'ሞጣ', 'ሙስሊም', 'ወንድም', 'ጎን', 'ቆመ', 'ጥሪ', 'አቀረበ']\n",
      "['ሙስልም', 'ህብረተሰብ', 'ጥል', 'ታወረ', 'ፔጅ', 'ብሎክ', 'ጊዜ']\n",
      "['ስራ', 'ጀመረ', 'ችግር', 'ተሻገረ', 'አየ', 'ደስ']\n",
      "['ክንደብርቱ', 'በርታ', 'አሸነፈ', 'እርግጠኛ']\n",
      "['ዛሬ', 'ተሰማ', 'ታገተ', 'ተማሪ', 'መንግስት', 'ተፈታ', 'ትልቅ', 'ደስ', 'ወንጀል', 'ፈጽሞ', 'ደገመ', 'መንግስት', 'ወንጀል', 'ፍርድ', 'አቀረበ']\n",
      "['አብይ', 'ስምምነት', 'ፈረመ', 'አብይ', 'ብዙ', 'ብልጽግና', 'ተቃወመ', 'ጠላ', 'ግልጽ', 'ቋንቋ', 'ሌባ', 'ጥዩፍ', 'ሰራ', 'ተጋ', 'ሰነፍ', 'ቀጠለ', 'ቻለ', 'ቋመ', 'ያዘ', 'ጥሩ']\n"
     ]
    }
   ],
   "source": [
    "clean_doc=[]\n",
    "for x in my_doc:\n",
    "    clean_doc.append(stopWordRemoval(x))\n",
    "    print(stopWordRemoval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90a0a81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_lemma=make_n_grams(my_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "999f686b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In stage 0: transferred 28 clusters with 5 clusters populated\n",
      "In stage 1: transferred 22 clusters with 5 clusters populated\n",
      "In stage 2: transferred 23 clusters with 5 clusters populated\n",
      "In stage 3: transferred 28 clusters with 5 clusters populated\n",
      "In stage 4: transferred 29 clusters with 5 clusters populated\n",
      "In stage 5: transferred 21 clusters with 4 clusters populated\n",
      "In stage 6: transferred 20 clusters with 5 clusters populated\n",
      "In stage 7: transferred 25 clusters with 5 clusters populated\n",
      "In stage 8: transferred 20 clusters with 5 clusters populated\n"
     ]
    }
   ],
   "source": [
    "from gsdmm import MovieGroupProcess\n",
    "\n",
    "# cast tweets to numpy array\n",
    "docs = clean_doc\n",
    "#docs = processed_lemma\n",
    "# create dictionary of all words in all documents\n",
    "# initialize a Dictionary\n",
    "dictionary = gensim.corpora.Dictionary(docs)\n",
    "\n",
    "# filter extreme cases out of dictionary\n",
    "#dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
    "\n",
    "# create variable containing length of dictionary/vocab\n",
    "vocab_length = len(dictionary)\n",
    "\n",
    "# create BOW dictionary\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "\n",
    "# initialize GSDMM\n",
    "gsdmm = MovieGroupProcess(K=5, alpha=0.3, beta=0.8, n_iters=9)\n",
    "\n",
    "# fit GSDMM model\n",
    "y = gsdmm.fit(docs, vocab_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f5b50f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "07efade4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents per topic : [11  8  6  7  4]\n",
      "Most important clusters (by number of docs inside): [0 1 3 2 4]\n",
      "\n",
      "Cluster 0 : [('መሪ', 3), ('ሀገር', 2), ('ሙስሊም', 2), ('በረከት', 2), ('አሜን', 2), ('ደረሰ', 2), ('ደስ', 2), ('ሁሌም', 2), ('አሸናፊ', 2), ('ኢትዮጲያ', 1)]\n",
      "\n",
      "Cluster 1 : [('ሰላም', 3), ('መጣ', 2), ('ፈለገ', 2), ('ጸረ', 2), ('ተናገረ', 1), ('መልካም', 1), ('ችግር', 1), ('በታች', 1), ('ስሜት', 1), ('ማንም', 1)]\n",
      "\n",
      "Cluster 3 : [('ጥል', 2), ('ሰማ', 2), ('ቻለ', 2), ('ሰይጣን', 2), ('ተከላከለ', 2), ('ሙስሊም', 2), ('ሞጣ', 2), ('እውቀት', 1), ('ጥግ', 1), ('ተናገረ', 1)]\n",
      "\n",
      "Cluster 2 : [('አብይ', 5), ('ድል', 3), ('ፓርቲ', 2), ('የለም', 2), ('ብልጽግና', 2), ('አረገ', 2), ('ዲሞክራሲ', 2), ('መብት', 1), ('መሰረተ', 1), ('ጀዋር', 1)]\n",
      "\n",
      "Cluster 4 : [('ወንጀል', 3), ('ተማሪ', 3), ('መንግስት', 3), ('ሰራ', 2), ('ሰጠ', 2), ('ታገተ', 2), ('ዘር', 2), ('ተካ', 2), ('ነውር', 1), ('ስድብ', 1)]\n"
     ]
    }
   ],
   "source": [
    "word_cluster=[]\n",
    "cluster_index=[]\n",
    "# print number of documents per topic\n",
    "doc_count = np.array(gsdmm.cluster_doc_count)\n",
    "print('Number of documents per topic :', doc_count)\n",
    "\n",
    "# Topics sorted by the number of document they are allocated to\n",
    "top_index = doc_count.argsort()[-15:][::-1]\n",
    "print('Most important clusters (by number of docs inside):', top_index)\n",
    "\n",
    "# define function to get top words per topic\n",
    "def top_words(cluster_word_distribution, top_cluster, values):\n",
    "    for cluster in top_cluster:\n",
    "        sort_dicts = sorted(cluster_word_distribution[cluster].items(), key=lambda k: k[1], reverse=True)[:values]\n",
    "        word_cluster.append(sort_dicts)\n",
    "        cluster_index.append(cluster)\n",
    "        print(\"\\nCluster %s : %s\"%(cluster, sort_dicts))\n",
    "\n",
    "# get top words in topics\n",
    "top_words(gsdmm.cluster_word_distribution, top_index, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f2020938",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_to_word=pd.DataFrame({'index':cluster_index, 'words':word_cluster})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cd121659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4726745872279764\n"
     ]
    }
   ],
   "source": [
    "# import library from gensim  \n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# define function to get words in topics\n",
    "def get_topics_lists(model, top_clusters, n_words):\n",
    "    '''\n",
    "    Gets lists of words in topics as a list of lists.\n",
    "    \n",
    "    model: gsdmm instance\n",
    "    top_clusters:  numpy array containing indices of top_clusters\n",
    "    n_words: top n number of words to include\n",
    "    \n",
    "    '''\n",
    "    # create empty list to contain topics\n",
    "    topics = []\n",
    "    \n",
    "    # iterate over top n clusters\n",
    "    for cluster in top_clusters:\n",
    "        #create sorted dictionary of word distributions\n",
    "        sorted_dict = sorted(model.cluster_word_distribution[cluster].items(), key=lambda k: k[1], reverse=True)[:n_words]\n",
    "         \n",
    "        #create empty list to contain words\n",
    "        topic = []\n",
    "        \n",
    "        #iterate over top n words in topic\n",
    "        for k,v in sorted_dict:\n",
    "            #append words to topic list\n",
    "            topic.append(k)\n",
    "            \n",
    "        #append topics to topics list    \n",
    "        topics.append(topic)\n",
    "    \n",
    "    return topics\n",
    "\n",
    "# get topics to feed to coherence model\n",
    "topics = get_topics_lists(gsdmm, top_index, 50) \n",
    "\n",
    "# evaluate model using Topic Coherence score\n",
    "cm_gsdmm = CoherenceModel(topics=topics, \n",
    "                          model=gsdmm,\n",
    "                          dictionary=dictionary, \n",
    "                          corpus=bow_corpus, \n",
    "                          texts=docs, \n",
    "                          coherence='c_v')\n",
    "\n",
    "# get coherence value\n",
    "coherence_gsdmm = cm_gsdmm.get_coherence()  \n",
    "\n",
    "print(coherence_gsdmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4a0808ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data=pd.read_csv('C:/Users/Kois/Desktop/thesis/dataset/tweets_long/outTRAIN.csv')\n",
    "data=pd.read_csv('C:/Users/Kois/Downloads/100 sentences.csv')\n",
    "data=data[data['tweet']!='lost']\n",
    "#data.tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "55a7be68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#topic_to_word.loc[topic_to_word['index'] == 0]\n",
    "def create_topics_dataframe(data_text=data.tweet,  mgp=gsdmm, threshold=0.3, topic_dict=topics):\n",
    "    result = pd.DataFrame(columns=['Text', 'Topic','Words'])\n",
    "    for i, text in enumerate(data_text):\n",
    "        result.at[i, 'Text'] = text\n",
    "        prob = mgp.choose_best_label(clean_doc[i])\n",
    "        if prob[1] >= threshold:\n",
    "            result.at[i, 'Topic'] =prob[0]\n",
    "            _words=topic_to_word.loc[topic_to_word['index'] == prob[0]]\n",
    "            result.at[i, 'Words'] =_words['words']\n",
    "            #print(prob[0])\n",
    "            #result.at[i, 'Topic'] = topic_dict[prob[0]]\n",
    "        else:\n",
    "            result.at[i, 'Topic'] = 'Other'\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3b55ae83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>መብት የለም ብልጽግና አለ ፓርቲ መሰረተ</td>\n",
       "      <td>2</td>\n",
       "      <td>3    [(አብይ, 5), (ድል, 3), (ፓርቲ, 2), (የለም, 2), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ኢትዮጲያ መሪ ፓርቲ ብልጽና</td>\n",
       "      <td>0</td>\n",
       "      <td>0    [(መሪ, 3), (ሀገር, 2), (ሙስሊም, 2), (በረከት, 2),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>የት ደረሰ ልጅ</td>\n",
       "      <td>0</td>\n",
       "      <td>0    [(መሪ, 3), (ሀገር, 2), (ሙስሊም, 2), (በረከት, 2),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ምርጥ መሪ ደስ አለ ሀገር ስም ጠራ</td>\n",
       "      <td>0</td>\n",
       "      <td>0    [(መሪ, 3), (ሀገር, 2), (ሙስሊም, 2), (በረከት, 2),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>እውቀት ጥግ ሰማ ሰማ ተናገረ ለምን ታሪክ አነበበ ራስ ገመተ</td>\n",
       "      <td>3</td>\n",
       "      <td>2    [(ጥል, 2), (ሰማ, 2), (ቻለ, 2), (ሰይጣን, 2), (ተ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>አማራ ክልል ፈላ ልክ ገባ ሙስሊም ጠላላ ሙስሊም ኖረ ኦሮምያ ግንኙነት አሰበ</td>\n",
       "      <td>0</td>\n",
       "      <td>0    [(መሪ, 3), (ሀገር, 2), (ሙስሊም, 2), (በረከት, 2),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>በታች ስሜት፣ ማንም አክራሪ ሰከረ መጣ መስጂድ ሲያቃጥልብህ ተናገረ መልሰ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1    [(ሰላም, 3), (መጣ, 2), (ፈለገ, 2), (ጸረ, 2), (ተ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>አብይ ጀዋር ተጠቀመ አረገ ሁሉ ተጠያቂ አብይ</td>\n",
       "      <td>2</td>\n",
       "      <td>3    [(አብይ, 5), (ድል, 3), (ፓርቲ, 2), (የለም, 2), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>አብይ ሁሌም አሸናፊ ሁሌም ተሸላሚ ሀገር ወገን ሆነ ድንቅ በረከት አበረከ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0    [(መሪ, 3), (ሀገር, 2), (ሙስሊም, 2), (በረከት, 2),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ብዙ አወራ አሸናፊ አደረገ ኑሮ ጌታቸው ረዳ አፍሪካ ቀንድ ቁጥጥር ስር አዋለ</td>\n",
       "      <td>0</td>\n",
       "      <td>0    [(መሪ, 3), (ሀገር, 2), (ሙስሊም, 2), (በረከት, 2),...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text Topic  \\\n",
       "0                          መብት የለም ብልጽግና አለ ፓርቲ መሰረተ     2   \n",
       "1                                  ኢትዮጲያ መሪ ፓርቲ ብልጽና     0   \n",
       "2                                          የት ደረሰ ልጅ     0   \n",
       "3                             ምርጥ መሪ ደስ አለ ሀገር ስም ጠራ     0   \n",
       "4             እውቀት ጥግ ሰማ ሰማ ተናገረ ለምን ታሪክ አነበበ ራስ ገመተ     3   \n",
       "5   አማራ ክልል ፈላ ልክ ገባ ሙስሊም ጠላላ ሙስሊም ኖረ ኦሮምያ ግንኙነት አሰበ     0   \n",
       "6  በታች ስሜት፣ ማንም አክራሪ ሰከረ መጣ መስጂድ ሲያቃጥልብህ ተናገረ መልሰ...     1   \n",
       "7                       አብይ ጀዋር ተጠቀመ አረገ ሁሉ ተጠያቂ አብይ     2   \n",
       "8  አብይ ሁሌም አሸናፊ ሁሌም ተሸላሚ ሀገር ወገን ሆነ ድንቅ በረከት አበረከ...     0   \n",
       "9   ብዙ አወራ አሸናፊ አደረገ ኑሮ ጌታቸው ረዳ አፍሪካ ቀንድ ቁጥጥር ስር አዋለ     0   \n",
       "\n",
       "                                               Words  \n",
       "0  3    [(አብይ, 5), (ድል, 3), (ፓርቲ, 2), (የለም, 2), (...  \n",
       "1  0    [(መሪ, 3), (ሀገር, 2), (ሙስሊም, 2), (በረከት, 2),...  \n",
       "2  0    [(መሪ, 3), (ሀገር, 2), (ሙስሊም, 2), (በረከት, 2),...  \n",
       "3  0    [(መሪ, 3), (ሀገር, 2), (ሙስሊም, 2), (በረከት, 2),...  \n",
       "4  2    [(ጥል, 2), (ሰማ, 2), (ቻለ, 2), (ሰይጣን, 2), (ተ...  \n",
       "5  0    [(መሪ, 3), (ሀገር, 2), (ሙስሊም, 2), (በረከት, 2),...  \n",
       "6  1    [(ሰላም, 3), (መጣ, 2), (ፈለገ, 2), (ጸረ, 2), (ተ...  \n",
       "7  3    [(አብይ, 5), (ድል, 3), (ፓርቲ, 2), (የለም, 2), (...  \n",
       "8  0    [(መሪ, 3), (ሀገር, 2), (ሙስሊም, 2), (በረከት, 2),...  \n",
       "9  0    [(መሪ, 3), (ሀገር, 2), (ሙስሊም, 2), (በረከት, 2),...  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = create_topics_dataframe(data_text=data.tweet,  mgp=gsdmm, threshold=0.3, topic_dict=top_index)\n",
    "result.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9e4d8852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    [(ሰላም, 3), (መጣ, 2), (ፈለገ, 2), (ጸረ, 2), (ተናገረ, ...\n",
       "Name: words, dtype: object"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=['ሀገር', 'ጥቅም', 'ተሰለፈ', 'ሰጠ', 'መላጣ', 'ነአ', 'የሻቢይን', 'ይሁንታ', 'ተገኘ', 'ሀገር', 'ባህር', 'ሰጠ', 'ታምራት', 'እውነታ', 'ውርእ', 'እርር', 'መጣ', 'ሚሊዮን', 'አበረ']\n",
    "test2=['ዝም', 'ለካ','ብየ','ጠራ', 'ኤርትራ', 'የኢትዮጲያ','ወያናዎችና', 'ልጆጃቸው','የነብርኩ','የባህርዳርጎንደርኣዲስ']\n",
    "test3=['ድርሰት','ደገመ','አዲስ', 'ልብ', 'ውሀ', 'ድሮ', 'ካድሬዎች', 'ረሳ','ቀላል','ጭምር']\n",
    "test4=['ጎንደር', 'ዩኒቨርሲቲ', 'ተማሪዎችመምህራንና', 'አስተዳደር', 'ሰራተኛ', 'እርምጃ_ወሰደ']\n",
    "\n",
    "\n",
    "topic_index=gsdmm.choose_best_label(test)\n",
    "x=topic_to_word.loc[topic_to_word['index'] == topic_index[0]]\n",
    "x['words']\n",
    "#topic_index[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ce81cdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'finalized_gsdmm_model.pkl'\n",
    "pickle.dump(gsdmm, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6c17e270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0.25481473345539685)\n"
     ]
    }
   ],
   "source": [
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "result = loaded_model.choose_best_label(test4)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8dfb8f",
   "metadata": {},
   "source": [
    "# importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "\n",
    "# cast tweets to numpy array\n",
    "docs = processed_docs\n",
    "\n",
    "# create dictionary of all words in all documents\n",
    "dictionary = gensim.corpora.Dictionary(docs)\n",
    "\n",
    "# filter extreme cases out of dictionary\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
    "\n",
    "# create BOW dictionary\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "\n",
    "# create LDA model using preferred hyperparameters\n",
    "lda_model = gensim.models.LdaMulticore(bow_corpus, \n",
    "                                         num_topics=6, \n",
    "                                         id2word=dictionary, \n",
    "                                         passes=4, \n",
    "                                         workers=2,\n",
    "                                         random_state=21)\n",
    "\n",
    "# Save LDA model to disk\n",
    "path_to_model = \"C:\\\\Users\\\\Kois\\\\Desktop\\\\thesis\\\\dataset\\\\tweets_long\\\\model\\\\GSDMM.pkl\"\n",
    "lda_model.save(path_to_model)\n",
    "\n",
    "# for each topic, print words occuring in that topic\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41607019",
   "metadata": {},
   "source": [
    "# importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import hm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import library from gensim  \n",
    "from gensim.models import CoherenceModel\n",
    "from gsdmm import MovieGroupProcess\n",
    "\n",
    "my_doc=pd.read_csv('C:/Users/Kois/lemmatizedCorpus.csv') #Lemmatized tokens\n",
    "my_doc=my_doc.drop(['Unnamed: 0.1'], axis = 1)\n",
    "my_doc=my_doc.drop(['Unnamed: 0'], axis = 1)\n",
    "my_doc=my_doc.to_numpy().tolist()\n",
    "len(my_doc)\n",
    "my_doc\n",
    "\n",
    "new_list=[]\n",
    "temp_list=[]\n",
    "for x in my_doc:\n",
    "    for item in x:\n",
    "        if str(item) != 'nan':\n",
    "            temp_list.append(item)\n",
    "    new_list.append(temp_list)\n",
    "    temp_list=[]\n",
    "my_doc=new_list\n",
    "len(my_doc)\n",
    "\n",
    "stop_words=['እኔ','የእኔ','እኔራሴ','እኛ','የእኛ','እኛራሳችን','አንቺ','ነህ','አላችሁ','እርስዎ','ትፈልጋለህ','ያንተ','ራስህን','እራሳችሁ','እሱ','የእሱ','ራሱ','እሷ','እሷናት','የእሷ','እራሷ','ነው','እነሱ','እነሱን','የእነሱ','ራሳቸው','ምንድን','የትኛው','ማን','ይህ','የሚልነው','ያ','እነዚህ','እነዚያ','ነኝ','ናቸው','ነበር','ነበሩ','ሁን','ቆይቷል','መሆን','አላቸው','አለው','ነበረው','ያለው','መስራት','ያደርጋል','አደረገ','ማድረግ','ሀ','አንድ','የ','እና','ከሆነ','ወይም'\n",
    ",'ምክንያቱም','እንደ','እስከ','እያለ','በ','ለ','ጋር','ስለ','ላይ','መካከል','ወደ','በኩል','ወቅት','ከዚህበፊት','በኋላ','ከላይ','ከታች','ከ','ወደላይ','ታች','ውስጥ','ውጭ','በላይ','እንደገና','ተጨማሪ','ከዚያ','አንድጊዜ','እዚህ','እዚያ','መቼ','የት','ለምን','እንዴት','ሁሉም','ማንኛውም','ሁለቱም','እያንዳንዳቸው','ጥቂቶች','በጣም','ሌላ','አንዳንድ','እንደዚህ','ብቻ','የራሱ','ተመሳሳይ','ስለዚህ','ይልቅ','እንዲሁ','ት','ይችላል','ይገባል','ይገባኛል','አሁን','መ','ም','ኦ','ዳግም','መሆን','ሁለ','ሁለም','ሕዝብ','ሀሙስ','ለመሆኑ','ለምንድን','ሌሎች','መጽሀፍ','ማክሰኞ','ምን','ሰኞ','ሰው','ሲሆን','ስንት','ረቡእ','ቅዳሜ','በዚህ','ብላ','ነገር','አለ','አርብ','አንተ','አንዳንድ','ኢትዮጵያ','እሁድ','እናንተ','እንኳን','እግር','ከመሆን','ወይንም','ዋና','ዘንድ','የሚከተለው','ያኔ','ይኼው','ገጽ','እነርሱ','ን','ና','ዎች','ይጠበቃል','ብለዋል','ሆ','ሁሉ','አንቀጽ','እንደሆነ','በማይበልጥ','መሰረት','ሁኔታ','ይሆናል','ሆኖ','ከአንድ','በማናቸውም','ወር','ከአምስት','በሆነ','ከዚህ','የሆነ','ሀያ','ሆነ','በኊላ','በአንድ',\n",
    "'የሆኑ','ከአስራ','የሆነውን','መሆኑ','ሌላውን','ከሰባት','ለሌላ','አለበት','ሲል','ይሆናሉ','በሙሉ','አስራ','ቢሆንም','አንዱ','የሌላውን','ከሁለት','የሆኑትን','በሆኑ','ጀምሮ','በመሆን','ባለ','ይህንን','እንዲቆይ','ሌላው','የሚሆነው','በአንዱ','ሲባል','ሳለ','የሆነው','መሆናቸው','በዋና','በማቀድ','ጊዜና','ለዚህ','ሶስተኛ','የነገሩ','ስድስት'\n",
    ",'በሆነው','ይሁን','ከዚሁ','በእነዚህ','ከማናቸውም','ከነበረው','በአንዳንድ','በእያንዳንዱ','ጊዜም','አስከ','የሌሎች','የሚሆኑት','ከሆነው','የነበረውን','ያሉ','ከሌሎች','አንዲት','ለሌሎች','ለሆነው','ሰኣት','ብሎ','ከሰላሳ','የሚሆኑ','ላይም','የሆናል','ከነዚህ','ያህል','ከሆነና','ለሆኑት','እነዚሁ','እንደሆኑ','ስለማናቸውም','ስለዚሁ','ከአንዳንድ','በእነዚሁ','በአምስት','የሆኑበታል','ለነዚህ','ለማንኛውም','አንደኛ','ይኸኛው','ከርሱ','መሆኑን','ለዚያው','ለዚሁ','ለእነርሱም','እዚሁ','ሐ','ረ','ሸ','አምስት','ከሶስት','በተለይም','በሌላ','ሺህ','ማናቸውንም','ከአስር','የማይበልጥ','እንዲሁም','ይህን','የዚህ','ማናቸውም','ከስድስት','መቶ','ያለ','አንድን','ያላቸውን','ሊሆን','ሶስት'\n",
    ",'ካልሆነ','ቢያንስ','ቢሆን','እነዚህን','አንዱን','ይሄ','ሁለት','ወይዘሮ','ተብሎ','ሳይሆን','እንደሆነና','ከብር','ሆኖም','የነበሩ',\n",
    "'የሌላ','ያላቸው','ይህንኑ','ሆነው','በስተቀር','ስም','እንደገና','የማያንስ','እጅግ','እንዲሆን','እንኳ','ከሀያ','ከሀምሳ','ይኸው','ለአንድ','የሚችለውን','በሚገባ','ይህም','እንዲሆኑ','ከሌላ','ለሆነ','በሌሎች','አንደሆነ','እንዲህ',\n",
    "'በነዚሁ','በእንደዚህ','ስምንት','ሲሆንና','ነዉ','ምንጊዜም','ለማናቸውም','የአንድ','እነዚህኑ','ሲሆኑ','በሁለቱም','እንደነዚህ','የሆኑት','የማናቸውም','ይህንንም','የአንድን','በሙሉም','በነዚህ','የዚሁ','ለእያንዳንዱ','ስለሆነ','መሆናቸውን','ማንኛውንም','ሁለቱ','እንጂ','ከስምንት','ሁለቱንም','በሁለት','በእስር','በሚል','ቁጥር','ባሉ','ከመቶ','እነዚህም','ሲኖር','ሰላሳ','ለሆኑ','ሰባት','እነደሆነ','ይህችው','ከእነዚህ','ከእነዚሁ','የአንቀጹ','ወይ','የሆነችን']\n",
    "\n",
    "\n",
    "def stopWordRemoval(tokens):\n",
    "    token_new=[]\n",
    "    for x in tokens:\n",
    "        if x not in stop_words:\n",
    "            token_new.append(x)\n",
    "    return token_new\n",
    "\n",
    "clean_doc=[]\n",
    "for x in my_doc:\n",
    "    clean_doc.append(stopWordRemoval(x))\n",
    "#     print(stopWordRemoval(x))\n",
    "\n",
    "# cast tweets to numpy array\n",
    "docs = clean_doc\n",
    "#docs = processed_lemma\n",
    "# create dictionary of all words in all documents\n",
    "# initialize a Dictionary\n",
    "dictionary = gensim.corpora.Dictionary(docs)\n",
    "\n",
    "# filter extreme cases out of dictionary\n",
    "#dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
    "\n",
    "# create variable containing length of dictionary/vocab\n",
    "vocab_length = len(dictionary)\n",
    "\n",
    "# create BOW dictionary\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "\n",
    "# define function to get words in topics\n",
    "def get_topics_lists(model, top_clusters, n_words):\n",
    "    '''\n",
    "    Gets lists of words in topics as a list of lists.\n",
    "    \n",
    "    model: gsdmm instance\n",
    "    top_clusters:  numpy array containing indices of top_clusters\n",
    "    n_words: top n number of words to include\n",
    "    \n",
    "    '''\n",
    "    # create empty list to contain topics\n",
    "    topics = []\n",
    "    \n",
    "    # iterate over top n clusters\n",
    "    for cluster in top_clusters:\n",
    "        #create sorted dictionary of word distributions\n",
    "        sorted_dict = sorted(model.cluster_word_distribution[cluster].items(), key=lambda k: k[1], reverse=True)[:n_words]\n",
    "         \n",
    "        #create empty list to contain words\n",
    "        topic = []\n",
    "        \n",
    "        #iterate over top n words in topic\n",
    "        for k,v in sorted_dict:\n",
    "            #append words to topic list\n",
    "            topic.append(k)\n",
    "            \n",
    "        #append topics to topics list    \n",
    "        topics.append(topic)\n",
    "    \n",
    "    return topics\n",
    "result = {}\n",
    "for k_ in range(2,11):\n",
    "    for alpha_ in range(0, 11, 1):\n",
    "        for beta_ in range(0,11, 1):\n",
    "            gsdmm = MovieGroupProcess(K=k_, alpha=alpha_/10, beta=beta_/10, n_iters=9)\n",
    "            gsdmm.fit(docs, vocab_length)\n",
    "            # print number of documents per topic\n",
    "            doc_count = np.array(gsdmm.cluster_doc_count)\n",
    "            # print('Number of documents per topic :', doc_count)\n",
    "            \n",
    "            # Topics sorted by the number of document they are allocated to\n",
    "#             print(doc_count)\n",
    "            top_index = doc_count.argsort()[-15:][::-1]\n",
    "            # get topics to feed to coherence model\n",
    "            topics = get_topics_lists(gsdmm, top_index, 50) \n",
    "\n",
    "            # evaluate model using Topic Coherence score\n",
    "            cm_gsdmm = CoherenceModel(topics=topics, \n",
    "                                      model=gsdmm,\n",
    "                                      dictionary=dictionary, \n",
    "                                      corpus=bow_corpus, \n",
    "                                      texts=docs, \n",
    "                                      coherence='c_v')\n",
    "\n",
    "            # get coherence value\n",
    "            coherence_gsdmm = cm_gsdmm.get_coherence()  \n",
    "            if coherence_gsdmm >= 0.4:\n",
    "                print((k_,alpha_/10,beta_/10))\n",
    "            result[(k_, alpha_/10,beta_/10)] = coherence_gsdmm\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5224da7",
   "metadata": {},
   "source": [
    "def compute_coherence_values(dictionary, doc_term_matrix, start, stop, step):\n",
    "    \"\"\"\n",
    "    Input   : dictionary : Gensim dictionary\n",
    "              corpus : Gensim corpus\n",
    "              texts : List of input texts\n",
    "              stop : Max num of topics\n",
    "    purpose : Compute c_v coherence for various number of topics\n",
    "    Output  : model_list : List of LSA topic models\n",
    "              coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, stop, step):\n",
    "        gsdmm = MovieGroupProcess(K=num_topics, alpha=0.1, beta=0.9, n_iters=9)\n",
    "        # fit GSDMM model\n",
    "        y = gsdmm.fit(docs, vocab_length)\n",
    "        topics = get_topics_lists(gsdmm, top_index, 45) \n",
    "        cm_gsdmm = CoherenceModel(model=gsdmm,topics=topics, \n",
    "                          dictionary=dictionary, \n",
    "                          corpus=bow_corpus, \n",
    "                          texts=docs, \n",
    "                          coherence='c_v')\n",
    "        coherence_values.append(cm_gsdmm.get_coherence())\n",
    "    return coherence_values\n",
    "    #return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706829a3",
   "metadata": {},
   "source": [
    "def plot_graph(doc_clean,start, stop, step):\n",
    "    coherence_values = compute_coherence_values(dictionary, bow_corpus, start,stop, step)\n",
    "    # Show graph\n",
    "    x = range(start, stop, step)\n",
    "    plt.plot(x, coherence_values)\n",
    "    plt.xlabel(\"Number of Topics\")\n",
    "    plt.ylabel(\"Coherence score\")\n",
    "    plt.legend((\"coherence_values\"), loc='best')\n",
    "    plt.show()\n",
    "\n",
    "start,stop,step=2,10,1\n",
    "plot_graph(bow_corpus,start,stop,step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce34248",
   "metadata": {},
   "source": [
    "# import library from gensim  \n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# instantiate topic coherence model\n",
    "cm = CoherenceModel(model=lda_model, corpus=bow_corpus, texts=docs, coherence='c_v')\n",
    "\n",
    "# get topic coherence score\n",
    "coherence_lda = cm.get_coherence() \n",
    "print(coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d81e07",
   "metadata": {},
   "source": [
    "lemma_df = pd.DataFrame(processed_lemma) \n",
    "lemma_df.to_csv('LemmaTEST.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72602fc8",
   "metadata": {},
   "source": [
    "pip install git+https://github.com/atefm/pDMM.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a2929d",
   "metadata": {},
   "source": [
    "pip install git+https://github.com/rwalk/gsdmm.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7f8766",
   "metadata": {},
   "source": [
    "for x in my_doc:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e46088",
   "metadata": {},
   "source": [
    "conda install -c conda-forge wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947e0761",
   "metadata": {},
   "source": [
    "import hm\n",
    "listOfNouns=[]\n",
    "for x in processed_docs:\n",
    "    for l in x:\n",
    "        y=hm.anal('amh',l,um=True)\n",
    "        try:\n",
    "            if y[1]['POS']=='n':\n",
    "                listOfNouns.append(l)\n",
    "        except:\n",
    "            if y==[]:\n",
    "                print()\n",
    "            elif y[0]['POS']=='n':\n",
    "                listOfNouns.append(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8071cf5",
   "metadata": {},
   "source": [
    "listOfNouns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
